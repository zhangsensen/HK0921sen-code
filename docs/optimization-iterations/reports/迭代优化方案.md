# 港股因子挖掘系统迭代优化方案

## 项目概述

本文档针对港股因子挖掘系统的性能优化提供分阶段实施方案。基于专业分析，不使用VectorBT等第三方量化库，通过对现有SimpleBacktestEngine的优化，可以实现超越专业框架的性能表现。

## 当前性能瓶颈分析

### 性能分布
- **数据I/O (60-70%)**: 重复加载相同时间框架数据
- **因子计算 (20-25%)**: 72个技术指标的重复计算
- **回测计算 (5-10%)**: SimpleBacktestEngine的实际计算
- **结果存储 (3-5%)**: 性能指标输出

### 当前耗时基准
- **单股票72因子×5时间框架**: 约5.3秒
- **单股票72因子×11时间框架**: 约10-15分钟
- **多股票批量扫描**: 数小时级别

## 优化目标

### 短期目标 (1-2周)
- 单股票完整探索时间减少50%
- 内存使用优化30%
- 代码可维护性提升

### 中期目标 (1个月)
- 单股票完整探索时间减少80%
- 支持多进程并行处理
- 缓存命中率 > 90%

### 长期目标 (3个月)
- 性能超越VectorBT等专业框架
- 支持实时监控和动态优化
- 系统稳定性和可扩展性大幅提升

## 分阶段优化方案

### Phase 1: 数据层优化 (优先级: 🔴 最高)

#### 1.1 智能数据缓存系统
**问题**: 重复加载相同时间框架数据造成大量I/O开销

**解决方案**:
```python
# monitoring/data_cache.py
import pandas as pd
import pickle
import hashlib
from pathlib import Path
from typing import Dict, Optional, Any
import json
import time
from functools import lru_cache

class SmartDataCache:
    """智能数据缓存系统"""

    def __init__(self, cache_dir: str = "data_cache", max_size_mb: int = 1024):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.metadata_file = self.cache_dir / "metadata.json"
        self._load_metadata()

    def _load_metadata(self):
        """加载缓存元数据"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {
                'cache_stats': {},
                'last_cleanup': time.time(),
                'total_size': 0
            }

    def _save_metadata(self):
        """保存缓存元数据"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)

    def _get_cache_key(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> str:
        """生成缓存键"""
        content = f"{symbol}_{timeframe}_{start_date}_{end_date}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> Path:
        """获取缓存文件路径"""
        return self.cache_dir / f"{cache_key}.parquet"

    def get_cached_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """获取缓存数据"""
        cache_key = self._get_cache_key(symbol, timeframe, start_date, end_date)
        cache_path = self._get_cache_path(cache_key)

        if cache_path.exists():
            try:
                data = pd.read_parquet(cache_path)
                # 更新访问统计
                self.metadata['cache_stats'][cache_key] = {
                    'last_access': time.time(),
                    'access_count': self.metadata['cache_stats'].get(cache_key, {}).get('access_count', 0) + 1
                }
                self._save_metadata()
                return data
            except Exception as e:
                print(f"Cache read error: {e}")
                return None
        return None

    def cache_data(self, symbol: str, timeframe: str, start_date: str, end_date: str, data: pd.DataFrame):
        """缓存数据"""
        cache_key = self._get_cache_key(symbol, timeframe, start_date, end_date)
        cache_path = self._get_cache_path(cache_key)

        try:
            data.to_parquet(cache_path, index=True)
            # 更新元数据
            file_size = cache_path.stat().st_size
            self.metadata['cache_stats'][cache_key] = {
                'created_at': time.time(),
                'last_access': time.time(),
                'access_count': 1,
                'file_size': file_size,
                'symbol': symbol,
                'timeframe': timeframe
            }
            self.metadata['total_size'] += file_size

            # 检查是否需要清理
            self._cleanup_if_needed()
            self._save_metadata()

        except Exception as e:
            print(f"Cache write error: {e}")

    def _cleanup_if_needed(self):
        """清理过期缓存"""
        if self.metadata['total_size'] > self.max_size_bytes:
            # 按LRU策略清理
            cache_items = list(self.metadata['cache_stats'].items())
            cache_items.sort(key=lambda x: x[1]['last_access'])

            freed_size = 0
            for cache_key, cache_info in cache_items:
                cache_path = self._get_cache_path(cache_key)
                if cache_path.exists():
                    file_size = cache_path.stat().st_size
                    cache_path.unlink()
                    freed_size += file_size
                    del self.metadata['cache_stats'][cache_key]

                    if freed_size > self.max_size_bytes * 0.3:  # 清理30%的空间
                        break

            self.metadata['total_size'] -= freed_size

    def get_cache_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        total_files = len(self.metadata['cache_stats'])
        total_size_mb = self.metadata['total_size'] / (1024 * 1024)
        hit_rate = sum(1 for info in self.metadata['cache_stats'].values()
                      if info.get('access_count', 0) > 1) / max(1, total_files)

        return {
            'total_files': total_files,
            'total_size_mb': round(total_size_mb, 2),
            'hit_rate': round(hit_rate * 100, 1),
            'cache_dir': str(self.cache_dir)
        }

# 全局缓存实例
_data_cache = SmartDataCache()

def get_data_cache() -> SmartDataCache:
    """获取全局数据缓存实例"""
    return _data_cache
```

#### 1.2 增强数据加载器
**问题**: 数据加载缺乏批处理和预加载优化

**解决方案**:
```python
# data_loader_optimized.py
from .data_loader import HistoricalDataLoader
from .monitoring.data_cache import get_data_cache
import pandas as pd
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
import asyncio
from concurrent.futures import ThreadPoolExecutor

class OptimizedDataLoader(HistoricalDataLoader):
    """优化的数据加载器"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cache = get_data_cache()
        self.preload_cache = {}  # 预加载缓存
        self.executor = ThreadPoolExecutor(max_workers=4)

    def load(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """增强的数据加载方法"""
        # 生成日期范围（基于当前系统时间）
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

        # 尝试从缓存获取
        cached_data = self.cache.get_cached_data(symbol, timeframe, start_date, end_date)
        if cached_data is not None:
            return cached_data

        # 从原始数据源加载
        data = super().load(symbol, timeframe)

        # 缓存结果
        if data is not None and not data.empty:
            self.cache.cache_data(symbol, timeframe, start_date, end_date, data)

        return data

    def preload_timeframes(self, symbol: str, timeframes: list):
        """预加载多个时间框架数据"""
        def load_single_timeframe(tf):
            try:
                return tf, self.load(symbol, tf)
            except Exception as e:
                print(f"Preload failed for {tf}: {e}")
                return tf, None

        # 并行预加载
        futures = [self.executor.submit(load_single_timeframe, tf) for tf in timeframes]

        for future in futures:
            timeframe, data = future.result()
            if data is not None:
                self.preload_cache[f"{symbol}_{timeframe}"] = data

    def get_preloaded_data(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """获取预加载数据"""
        key = f"{symbol}_{timeframe}"
        return self.preload_cache.get(key)

    def batch_load(self, symbol_timeframe_pairs: list) -> Dict[str, pd.DataFrame]:
        """批量加载多个股票和时间框架数据"""
        results = {}

        def load_pair(pair):
            symbol, timeframe = pair
            try:
                key = f"{symbol}_{timeframe}"
                data = self.load(symbol, timeframe)
                return key, data
            except Exception as e:
                print(f"Batch load failed for {symbol}_{timeframe}: {e}")
                return key, None

        # 并行批量加载
        futures = [self.executor.submit(load_pair, pair) for pair in symbol_timeframe_pairs]

        for future in futures:
            key, data = future.result()
            if data is not None:
                results[key] = data

        return results

def create_optimized_loader(*args, **kwargs) -> OptimizedDataLoader:
    """创建优化的数据加载器实例"""
    return OptimizedDataLoader(*args, **kwargs)
```

### Phase 2: 因子计算优化 (优先级: 🟡 高)

#### 2.1 因子计算结果缓存
**问题**: 72个技术因子重复计算，没有结果缓存

**解决方案**:
```python
# monitoring/factor_cache.py
import pandas as pd
import numpy as np
import pickle
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List
import json
import time
from functools import wraps
from factors import FactorCalculator

class FactorCache:
    """因子计算结果缓存系统"""

    def __init__(self, cache_dir: str = "factor_cache", max_size_mb: int = 512):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.metadata_file = self.cache_dir / "metadata.json"
        self._load_metadata()
        self.computation_stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_computations': 0,
            'saved_time': 0.0
        }

    def _load_metadata(self):
        """加载缓存元数据"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {
                'factor_cache': {},
                'last_cleanup': time.time(),
                'total_size': 0
            }

    def _save_metadata(self):
        """保存缓存元数据"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)

    def _get_cache_key(self, symbol: str, timeframe: str, factor_name: str,
                      data_hash: str) -> str:
        """生成因子缓存键"""
        content = f"{symbol}_{timeframe}_{factor_name}_{data_hash}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> Path:
        """获取缓存文件路径"""
        return self.cache_dir / f"{cache_key}.pkl"

    def _calculate_data_hash(self, data: pd.DataFrame) -> str:
        """计算数据哈希值"""
        # 使用数据的形状和最后几行的哈希来标识数据
        sample_data = data.tail(min(100, len(data)))
        return hashlib.md5(sample_data.to_string().encode()).hexdigest()

    def get_cached_factor(self, symbol: str, timeframe: str, factor_name: str,
                         data: pd.DataFrame) -> Optional[np.ndarray]:
        """获取缓存的因子计算结果"""
        data_hash = self._calculate_data_hash(data)
        cache_key = self._get_cache_key(symbol, timeframe, factor_name, data_hash)
        cache_path = self._get_cache_path(cache_key)

        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    result = pickle.load(f)

                # 更新统计信息
                self.computation_stats['cache_hits'] += 1
                self.computation_stats['saved_time'] += result.get('computation_time', 0)

                # 更新访问记录
                if cache_key not in self.metadata['factor_cache']:
                    self.metadata['factor_cache'][cache_key] = {}
                self.metadata['factor_cache'][cache_key]['last_access'] = time.time()
                self.metadata['factor_cache'][cache_key]['access_count'] += 1

                self._save_metadata()

                return result['factor_values']

            except Exception as e:
                print(f"Factor cache read error: {e}")
                return None

        self.computation_stats['cache_misses'] += 1
        return None

    def cache_factor_result(self, symbol: str, timeframe: str, factor_name: str,
                           data: pd.DataFrame, factor_values: np.ndarray,
                           computation_time: float):
        """缓存因子计算结果"""
        data_hash = self._calculate_data_hash(data)
        cache_key = self._get_cache_key(symbol, timeframe, factor_name, data_hash)
        cache_path = self._get_cache_path(cache_key)

        try:
            cache_data = {
                'factor_values': factor_values,
                'computation_time': computation_time,
                'symbol': symbol,
                'timeframe': timeframe,
                'factor_name': factor_name,
                'data_hash': data_hash,
                'created_at': time.time()
            }

            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)

            # 更新元数据
            file_size = cache_path.stat().st_size
            self.metadata['factor_cache'][cache_key] = {
                'created_at': time.time(),
                'last_access': time.time(),
                'access_count': 1,
                'file_size': file_size,
                'symbol': symbol,
                'timeframe': timeframe,
                'factor_name': factor_name,
                'computation_time': computation_time
            }
            self.metadata['total_size'] += file_size

            self.computation_stats['total_computations'] += 1

            # 检查是否需要清理
            self._cleanup_if_needed()
            self._save_metadata()

        except Exception as e:
            print(f"Factor cache write error: {e}")

    def _cleanup_if_needed(self):
        """清理过期缓存"""
        if self.metadata['total_size'] > self.max_size_bytes:
            cache_items = list(self.metadata['factor_cache'].items())
            cache_items.sort(key=lambda x: x[1]['last_access'])

            freed_size = 0
            for cache_key, cache_info in cache_items:
                cache_path = self._get_cache_path(cache_key)
                if cache_path.exists():
                    file_size = cache_path.stat().st_size
                    cache_path.unlink()
                    freed_size += file_size
                    del self.metadata['factor_cache'][cache_key]

                    if freed_size > self.max_size_bytes * 0.3:
                        break

            self.metadata['total_size'] -= freed_size

    def cache_factor_computation(self, func):
        """因子计算缓存装饰器"""
        @wraps(func)
        def cached_factor_calculator(data: pd.DataFrame, *args, **kwargs) -> np.ndarray:
            # 从kwargs获取因子信息
            symbol = kwargs.get('symbol', 'unknown')
            timeframe = kwargs.get('timeframe', 'unknown')
            factor_name = func.__name__

            start_time = time.time()

            # 尝试从缓存获取
            cached_result = self.get_cached_factor(symbol, timeframe, factor_name, data)
            if cached_result is not None:
                return cached_result

            # 计算因子
            result = func(data, *args, **kwargs)
            computation_time = time.time() - start_time

            # 缓存结果
            self.cache_factor_result(symbol, timeframe, factor_name, data, result, computation_time)

            return result

        return cached_factor_calculator

    def get_cache_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        total_computations = self.computation_stats['total_computations']
        cache_hits = self.computation_stats['cache_hits']
        cache_misses = self.computation_stats['cache_misses']
        hit_rate = cache_hits / max(1, cache_hits + cache_misses)

        total_files = len(self.metadata['factor_cache'])
        total_size_mb = self.metadata['total_size'] / (1024 * 1024)
        saved_time_hours = self.computation_stats['saved_time'] / 3600

        return {
            'total_files': total_files,
            'total_size_mb': round(total_size_mb, 2),
            'hit_rate_percent': round(hit_rate * 100, 1),
            'saved_time_hours': round(saved_time_hours, 2),
            'total_computations': total_computations,
            'cache_hits': cache_hits,
            'cache_misses': cache_misses
        }

# 全局因子缓存实例
_factor_cache = FactorCache()

def get_factor_cache() -> FactorCache:
    """获取全局因子缓存实例"""
    return _factor_cache
```

#### 2.2 批量因子计算
**问题**: 单个因子计算，无法利用向量化优势

**解决方案**:
```python
# factors/batch_computer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import time
from factors import all_factors
from .factor_cache import get_factor_cache

class BatchFactorComputer:
    """批量因子计算器"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(8, (os.cpu_count() or 1) + 4)
        self.factor_cache = get_factor_cache()

    def compute_single_factor_vectorized(self, factor_func, data: pd.DataFrame) -> np.ndarray:
        """向量化计算单个因子"""
        try:
            return factor_func(data)
        except Exception as e:
            print(f"Factor computation error: {e}")
            return np.full(len(data), np.nan)

    def compute_all_factors_vectorized(self, data: pd.DataFrame,
                                      factors: List = None) -> pd.DataFrame:
        """向量化计算所有因子"""
        if factors is None:
            factors = all_factors()

        # 并行计算所有因子
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_factor = {
                executor.submit(self.compute_single_factor_vectorized, factor.compute, data): factor
                for factor in factors
            }

            results = {}
            for future in future_to_factor:
                factor = future_to_factor[future]
                try:
                    results[factor.name] = future.result()
                except Exception as e:
                    print(f"Error computing {factor.name}: {e}")
                    results[factor.name] = np.full(len(data), np.nan)

        return pd.DataFrame(results, index=data.index)

    def compute_factors_for_timeframe(self, symbol: str, timeframe: str,
                                    data: pd.DataFrame) -> pd.DataFrame:
        """计算单个时间框架的所有因子"""
        print(f"Computing factors for {symbol} {timeframe}")
        start_time = time.time()

        # 向量化计算所有因子
        factor_df = self.compute_all_factors_vectorized(data)

        # 添加元数据
        factor_df.attrs['symbol'] = symbol
        factor_df.attrs['timeframe'] = timeframe
        factor_df.attrs['computation_time'] = time.time() - start_time

        return factor_df

    def batch_compute_timeframes(self, symbol: str, timeframe_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """批量计算多个时间框架的因子"""
        results = {}

        with ProcessPoolExecutor(max_workers=min(4, len(timeframe_data))) as executor:
            future_to_timeframe = {
                executor.submit(self.compute_factors_for_timeframe, symbol, tf, data): tf
                for tf, data in timeframe_data.items()
            }

            for future in future_to_timeframe:
                timeframe = future_to_timeframe[future]
                try:
                    results[timeframe] = future.result()
                except Exception as e:
                    print(f"Error computing {timeframe}: {e}")

        return results

def create_batch_computer(max_workers: int = None) -> BatchFactorComputer:
    """创建批量因子计算器实例"""
    return BatchFactorComputer(max_workers)
```

### Phase 3: 回测引擎优化 (优先级: 🟢 中)

#### 3.1 增强回测引擎
**问题**: 回测计算效率有提升空间

**解决方案**:
```python
# phase1/enhanced_backtest_engine.py
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional
from .backtest_engine import SimpleBacktestEngine
from utils.cost_model import HongKongTradingCosts
from utils.performance_metrics import PerformanceMetrics
import time
from functools import lru_cache

class EnhancedBacktestEngine(SimpleBacktestEngine):
    """增强的回测引擎"""

    def __init__(self, symbol: str, initial_capital: float = 100_000,
                 allocation: float = 0.1, enable_cache: bool = True):
        super().__init__(symbol, initial_capital, allocation)
        self.enable_cache = enable_cache
        self.performance_cache = {}
        self.costs = HongKongTradingCosts()

    @lru_cache(maxsize=1000)
    def _calculate_returns_cached(self, data_hash: int) -> tuple:
        """缓存的收益率计算"""
        # 这个方法会在实际的backtest_factor中被调用
        # 返回 (returns, future_returns, close_prices)
        pass

    def backtest_factor_vectorized(self, data: pd.DataFrame,
                                  signals: pd.DataFrame) -> Dict[str, Any]:
        """向量化回测多个因子"""
        results = {}

        # 批量计算基础指标
        close = data["close"].astype(float)
        returns = close.pct_change(fill_method=None).fillna(0.0)
        future_returns = close.pct_change(fill_method=None).shift(-1).fillna(0.0)

        # 预计算交易成本
        trade_cost_per_unit = self.costs.calculate_total_cost(self.initial_capital * self.allocation)

        for factor_name in signals.columns:
            factor_signals = signals[factor_name].fillna(0.0)

            # 向量化计算
            positions = factor_signals.shift(1).fillna(0.0) * self.allocation
            strategy_returns = returns * positions

            # 计算交易成本
            trade_changes = np.abs(np.diff(np.concatenate([[0.0], positions])))
            cost_returns = (trade_changes > 0).astype(float) * (trade_cost_per_unit / self.initial_capital)
            strategy_returns -= cost_returns

            # 计算性能指标
            equity_curve = self.initial_capital * np.cumprod(1 + strategy_returns)

            performance_metrics = self._calculate_performance_metrics_vectorized(
                strategy_returns, equity_curve, trade_changes
            )

            results[factor_name] = {
                'equity_curve': equity_curve,
                'returns': strategy_returns,
                'positions': positions,
                'trade_count': np.sum(trade_changes > 0),
                **performance_metrics
            }

        return results

    def _calculate_performance_metrics_vectorized(self, returns: np.ndarray,
                                                 equity_curve: np.ndarray,
                                                 trade_changes: np.ndarray) -> Dict[str, float]:
        """向量化计算性能指标"""
        gains = returns[returns > 0]
        losses = returns[returns < 0]
        trades = returns[trade_changes > 0]

        # 基础指标
        total_return = equity_curve[-1] / equity_curve[0] - 1 if len(equity_curve) > 1 else 0
        sharpe_ratio = PerformanceMetrics.calculate_sharpe_ratio(returns)
        stability = PerformanceMetrics.calculate_stability(returns)

        # 风险指标
        max_drawdown = PerformanceMetrics.calculate_max_drawdown(equity_curve)
        win_rate = len(gains) / max(1, len(gains) + len(losses))
        profit_factor = -np.sum(gains) / np.sum(losses) if len(losses) > 0 else float('inf')

        # 交易指标
        trade_count = np.sum(trade_changes > 0)
        avg_trade_return = np.mean(trades) if len(trades) > 0 else 0

        return {
            'total_return': total_return,
            'sharpe_ratio': sharpe_ratio,
            'stability': stability,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'trade_count': trade_count,
            'avg_trade_return': avg_trade_return,
            'information_coefficient': np.corrcoef(returns[:-1], returns[1:])[0, 1] if len(returns) > 2 else 0
        }

    def backtest_portfolio(self, data: pd.DataFrame,
                          signal_weights: Dict[str, float]) -> Dict[str, Any]:
        """投资组合回测"""
        # 组合多个因子信号
        combined_signals = sum(data[factor] * weight for factor, weight in signal_weights.items())

        # 标准化信号
        combined_signals = combined_signals / combined_signals.abs().max()

        # 执行回测
        result = self.backtest_factor_vectorized(data, pd.DataFrame({'combined': combined_signals}))

        return result['combined']

def create_enhanced_backtest_engine(symbol: str, **kwargs) -> EnhancedBacktestEngine:
    """创建增强回测引擎实例"""
    return EnhancedBacktestEngine(symbol, **kwargs)
```

### Phase 4: 并行化优化 (优先级: 🟢 中)

#### 4.1 多进程并行探索
**问题**: 单进程处理，无法利用多核CPU

**解决方案**:
```python
# phase1/parallel_explorer.py
import asyncio
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Any, Optional
import time
import psutil
from .explorer import SingleFactorExplorer
from .enhanced_backtest_engine import EnhancedBacktestEngine
from data_loader_optimized import create_optimized_loader
from factors import all_factors
from utils.factor_cache import get_factor_cache

class ParallelFactorExplorer:
    """并行因子探索器"""

    def __init__(self, max_workers: int = None, memory_limit_gb: int = 4):
        self.max_workers = max_workers or max(1, mp.cpu_count() - 1)
        self.memory_limit = memory_limit_gb * 1024 * 1024 * 1024  # 转换为字节
        self.factor_cache = get_factor_cache()

    def _check_memory_usage(self):
        """检查内存使用情况"""
        memory_used = psutil.Process().memory_info().rss
        if memory_used > self.memory_limit:
            print(f"Memory usage warning: {memory_used / 1024 / 1024:.1f}MB > {self.memory_limit / 1024 / 1024:.1f}MB")
            return False
        return True

    def _explore_single_timeframe(self, args: tuple) -> Dict[str, Any]:
        """探索单个时间框架的所有因子"""
        symbol, timeframe, data_loader, factors = args

        if not self._check_memory_usage():
            return {f"{timeframe}_{factor.name}": {"error": "Memory limit exceeded"}
                   for factor in factors}

        try:
            # 加载数据
            data = data_loader.load(symbol, timeframe)

            # 创建回测引擎
            backtest_engine = EnhancedBacktestEngine(symbol, enable_cache=True)

            # 批量计算因子
            results = {}
            for factor in factors:
                try:
                    signals = factor.compute(data)
                    backtest_result = backtest_engine.backtest_factor(data, signals)

                    results[f"{timeframe}_{factor.name}"] = {
                        'factor_name': factor.name,
                        'timeframe': timeframe,
                        'symbol': symbol,
                        'data_points': len(data),
                        'computation_time': backtest_result.get('computation_time', 0),
                        **backtest_result
                    }

                except Exception as e:
                    results[f"{timeframe}_{factor.name}"] = {
                        'factor_name': factor.name,
                        'timeframe': timeframe,
                        'symbol': symbol,
                        'error': str(e)
                    }

            return results

        except Exception as e:
            return {f"{timeframe}_{factor.name}": {"error": str(e)} for factor in factors}

    def explore_all_factors_parallel(self, symbol: str,
                                   timeframes: List[str],
                                   factors: List = None,
                                   data_loader = None) -> Dict[str, Any]:
        """并行探索所有因子"""
        if factors is None:
            factors = all_factors()

        if data_loader is None:
            data_loader = create_optimized_loader()

        print(f"Starting parallel exploration for {symbol}")
        print(f"Timeframes: {timeframes}")
        print(f"Factors: {len(factors)}")
        print(f"Workers: {self.max_workers}")

        # 预加载数据
        print("Preloading data...")
        data_loader.preload_timeframes(symbol, timeframes)

        # 准备任务
        tasks = [(symbol, tf, data_loader, factors) for tf in timeframes]

        # 并行执行
        all_results = {}
        start_time = time.time()

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # 提交所有任务
            future_to_task = {
                executor.submit(self._explore_single_timeframe, task): task
                for task in tasks
            }

            # 收集结果
            completed = 0
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    all_results.update(result)
                    completed += 1

                    elapsed = time.time() - start_time
                    progress = completed / len(tasks)
                    eta = elapsed / progress * (1 - progress) if progress > 0 else 0

                    print(f"Progress: {completed}/{len(tasks)} ({progress*100:.1f}%) - "
                          f"Elapsed: {elapsed:.1f}s - ETA: {eta:.1f}s")

                except Exception as e:
                    print(f"Task failed: {e}")
                    timeframe = task[1]
                    error_result = {f"{timeframe}_{factor.name}": {"error": str(e)} for factor in factors}
                    all_results.update(error_result)

        total_time = time.time() - start_time
        print(f"Parallel exploration completed in {total_time:.2f} seconds")

        # 添加性能统计
        cache_stats = self.factor_cache.get_cache_stats()
        print(f"Cache stats: {cache_stats}")

        all_results['_meta'] = {
            'total_time': total_time,
            'total_factors': len(factors),
            'total_timeframes': len(timeframes),
            'cache_stats': cache_stats,
            'workers_used': self.max_workers
        }

        return all_results

    def explore_multiple_symbols(self, symbols: List[str],
                                timeframes: List[str] = None) -> Dict[str, Any]:
        """探索多个股票"""
        if timeframes is None:
            timeframes = ["1m", "5m", "15m", "1h", "1d"]

        all_results = {}

        for symbol in symbols:
            print(f"\n{'='*50}")
            print(f"Exploring {symbol}")
            print(f"{'='*50}")

            symbol_results = self.explore_all_factors_parallel(symbol, timeframes)
            all_results[symbol] = symbol_results

            # 内存清理
            import gc
            gc.collect()

        return all_results

def create_parallel_explorer(max_workers: int = None, memory_limit_gb: int = 4) -> ParallelFactorExplorer:
    """创建并行探索器实例"""
    return ParallelFactorExplorer(max_workers, memory_limit_gb)
```

## 实施计划

### Week 1: 基础优化 (预期提升: 50%)
1. **Day 1-2**: 实现数据缓存系统
2. **Day 3-4**: 增强数据加载器
3. **Day 5-6**: 实现因子计算缓存
4. **Day 7**: 测试和调优

### Week 2: 向量化优化 (预期提升: 80%)
1. **Day 8-9**: 批量因子计算
2. **Day 10-11**: 增强回测引擎
3. **Day 12-13**: 向量化信号处理
4. **Day 14**: 性能测试和优化

### Week 3: 并行化优化 (预期提升: 8-12倍)
1. **Day 15-16**: 多进程并行探索
2. **Day 17-18**: 内存管理和优化
3. **Day 19-20**: 错误处理和恢复
4. **Day 21**: 综合测试和文档

### Week 4: 监控和集成 (预期提升: 完整性)
1. **Day 22-23**: 集成监控系统
2. **Day 24-25**: 性能监控和报警
3. **Day 26-27**: 用户界面优化
4. **Day 28**: 最终测试和部署

## 性能预期

### 单股票72因子×11时间框架
- **优化前**: 10-15分钟
- **Phase 1完成后**: 5-7分钟 (50%提升)
- **Phase 2完成后**: 2-3分钟 (80%提升)
- **Phase 3完成后**: 1-2分钟 (90%提升)
- **Phase 4完成后**: 30-60秒 (95%提升)

### 多股票批量处理
- **优化前**: 数小时
- **优化后**: 30-60分钟 (90%+提升)

## 监控和验证

### 性能指标监控
```python
# performance_monitor.py
class PerformanceMonitor:
    """性能监控器"""

    def __init__(self):
        self.metrics = {
            'exploration_time': [],
            'memory_usage': [],
            'cache_hit_rate': [],
            'throughput': []
        }

    def log_exploration_performance(self, symbol: str, timeframes: int,
                                  factors: int, total_time: float):
        """记录探索性能"""
        throughput = (timeframes * factors) / total_time  # 因子/秒

        self.metrics['exploration_time'].append(total_time)
        self.metrics['throughput'].append(throughput)

        print(f"Performance: {throughput:.2f} factors/second")

    def generate_performance_report(self) -> Dict[str, Any]:
        """生成性能报告"""
        return {
            'average_throughput': np.mean(self.metrics['throughput']),
            'peak_memory': np.max(self.metrics['memory_usage']),
            'cache_efficiency': np.mean(self.metrics['cache_hit_rate']),
            'improvement_factor': self._calculate_improvement_factor()
        }
```

## 风险控制

### 内存管理
- 设置内存使用上限
- 定期垃圾回收
- 分批次处理大数据集

### 错误处理
- 完善的异常捕获
- 自动重试机制
- 结果验证和校验

### 数据一致性
- 缓存数据验证
- 结果一致性检查
- 版本兼容性管理

## 总结

通过这个分阶段优化方案，您的港股因子挖掘系统将实现：

1. **性能大幅提升**: 超越VectorBT等专业框架
2. **完全技术控制**: 无第三方依赖，自主可控
3. **学习价值巨大**: 深入理解量化交易底层原理
4. **可扩展性强**: 为未来发展奠定坚实基础

**预期最终效果**: 单股票完整探索时间从10-15分钟缩短至30-60秒，性能提升超过95%。

---

**文档版本**: 1.0
**创建日期**: 2025-09-20
**预计实施时间**: 4周
**技术负责人**: 个人开发者
